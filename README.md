# üêç Mastering Python Libraries: An Interactive Exploration Guide
*From first import to production-ready code: a systematic approach to understanding any Python package*

**Author**: Gregory Damiani / [https://github.com/grggls](https://github.com/grggls)


**License**: CC BY-NC-SA (Attribution-NonCommercial-ShareAlike)

---

When you encounter a new Python library, interactive exploration becomes your most powerful learning tool. Whether you're using a Python REPL, IPython shell, or Jupyter notebook, the ability to experiment, inspect, and iterate in real-time transforms how quickly you can become productive with unfamiliar code.

The REPL (Read-Eval-Print Loop) excels at rapid experimentation and immediate feedback - perfect for testing hypotheses about how a library works. Jupyter notebooks complement this by serving as living documents where you can capture your exploration journey, document discoveries, and build reusable examples that become references for your team.

This guide provides a systematic approach to library exploration that goes beyond basic `help()` commands. You'll learn to understand the domain model, discover usage patterns, and write defensive, idiomatic code. The key insight is working backwards from your end goal: you want to understand not just *what* the library does, but *how* it thinks about the problem domain and *how* you should interact with it safely and efficiently.

---

## üìù Language and Conventions

Before we dive into exploration techniques, let's clarify some terminology that often causes confusion, even among experienced Python developers.

**Module**: A single Python file (`.py`) containing Python code. For example, `math.py` is a module you can import with `import math`. Modules are the basic building blocks of Python code organization.

**Package**: A collection of modules organized in a directory structure with an `__init__.py` file. The `__init__.py` file tells Python to treat the directory as a package. Most third-party code you install with `pip` is technically a package, even if it feels like a single unit.

**Library**: A conceptual term referring to a collection of code that provides specific functionality. It could be a single module, multiple modules, or a complex package structure. In everyday conversation, "library" and "package" are often used interchangeably when referring to third-party code.

**Examples in practice:**
- `requests` - Technically a package with multiple internal modules, but everyone calls it a "library"
- `json` - A single module in Python's standard library
- `numpy` - A complex package structure, commonly called a "library"

**Throughout this guide**, we'll use these terms somewhat interchangeably as they appear in real-world usage, but when precision matters, we'll be specific. The exploration techniques work the same regardless of whether you're examining a simple module or a complex package.

**Import conventions**: When exploring, you'll often see different import styles:
```python
import requests                        # Import the whole package
from requests import Session           # Import specific items into the local namespace
import requests as req                 # Import with alias
```

Each style affects what's available in your namespace and how you access the library's functionality, which influences your exploration approach.

This guide uses Python 3.8+ features and follows PEP 8 conventions. Code examples assume you're working in an interactive Python environment (IPython, Jupyter, or Python REPL).

### Key Conventions
- `module`: The library module being explored
- `obj`: Any Python object (class, function, instance)
- `name`: String name of an object
- `attr`: Attribute name as string
- `value`: Any Python value

### Import Patterns
```python
# Standard import for exploration
import module_name

# For deep inspection
import inspect
import importlib

# For testing and validation
import unittest
import doctest
```

## üîç Basic Introspection

Start your exploration with these fundamental introspection techniques. These commands give you the essential structure of any Python object - whether it's a module, class, or function. Think of this as taking an X-ray of the library to see its skeleton.

```python
import requests
from requests import Session

# Check what type of object you're working with
type(requests)     # <class 'module'>
type(Session)      # <class 'type'>

# List all available attributes
dir(requests)      # ['ConnectTimeout', 'HTTPError', 'Session', 'get', 'post', ...]

# Get the namespace as a dictionary (more detailed than dir)
vars(requests)     # {'__name__': 'requests', '__doc__': '...', 'get': <function>, ...}
```

### In IPython / Jupyter:
```ipython
# Enhanced introspection with ? and ??
requests?          # Show documentation and basic info
requests.get??     # Show source code (if available)
```

### Key insights from these commands:

`type()` tells you what kind of object you're dealing with, which affects how you can interact with it
`dir()` shows you everything available in the namespace - your roadmap to what's possible
`vars(`)` gives you the underlying dictionary representation with more detail than `dir()`
IPython's `?` gives you rich documentation, while `??` shows actual source code
Notice how `requests.Session` and `Session` refer to the same class after our imports

## üìö Documentation & Help

Good documentation is your roadmap to understanding a library's intended usage patterns. Python's built-in help system, combined with IPython's enhanced introspection, can reveal not just what functions do, but how the library authors expect you to use them.

```python
help(requests)             # View top-level docstring
help(requests.get)         # View docstring for function/class

# Rich documentation exploration
import webbrowser
webbrowser.open(f"https://pypi.org/project/{requests.__name__}/")
```

The top-level module docstring often contains the library's philosophy and high-level usage patterns. Individual function and class docstrings reveal the expected interfaces and common gotchas. IPython's `??` operator is particularly powerful because it shows you the actual implementation, helping you understand performance characteristics and edge cases.

---

## üîß Essential Dunder Attributes

Dunder (double underscore) attributes are Python's way of exposing metadata about objects. These attributes tell you about the object's identity, location, and interface contracts - essential information for understanding how to work with the library safely.

```python
requests.__doc__      # Docstring
requests.__file__     # Path to module file
requests.__name__     # Module name
requests.__version__  # Version (if available)
```

The `__all__` dunder lists all shared public strings of a module, function, library, etc. If you're writing your own module and sharing it, it's good practice to declare and export an `__all__` for your project. However, not all well-used community libraries do this unfortunately.

```python
os.__all__           # All exported strings of a function, module, etc.
```

```python
# For classes - understand the interface contract
requests.Response.__init__        # Constructor signature
requests.Response.__dict__        # Instance attributes
```

`__all__` is particularly important because it defines the public API - what the library authors consider safe and stable for external use. `__file__` helps you understand the library's structure and locate additional resources. For classes, `__slots__` tells you if the class uses memory optimization, which affects how you can dynamically add attributes.

## `__slots__` are unique interface detail that some packages use to optimize memory usage

`__slots__` is a special class attribute that tells Python to use a fixed-size array for storing instance attributes instead of a dynamic dictionary (`__dict__`). This optimization can significantly reduce memory usage and improve attribute access speed, especially for classes with many instances.

### Why Libraries Use `__slots__`

**Memory Optimization**: Classes with `__slots__` use 40-50% less memory per instance because they don't need to store a dictionary for each object.

**Performance**: Attribute access is faster since Python can directly access the slot instead of doing a dictionary lookup.

**Immutability**: Prevents adding new attributes after object creation, which can be useful for data classes and immutable objects.

### How to Explore `__slots__`

```python
# Check if a class uses __slots__
if hasattr(requests.Response, '__slots__'):
    print(f"Response uses __slots__: {requests.Response.__slots__}")
else:
    print("Response uses __dict__ for attributes")

# Compare memory usage (example with dataclasses)
from dataclasses import dataclass

@dataclass
class RegularPoint:
    x: int
    y: int

@dataclass(slots=True)
class SlottedPoint:
    x: int
    y: int

# RegularPoint.__slots__ -> AttributeError (uses __dict__)
# SlottedPoint.__slots__ -> ('x', 'y')

# Test attribute creation
regular = RegularPoint(1, 2)
regular.z = 3  # ‚úÖ Works

slotted = SlottedPoint(1, 2)
# slotted.z = 3  # ‚ùå AttributeError: 'SlottedPoint' object has no attribute 'z'
```

### Common Libraries That Use `__slots__`

**Standard Library Examples:**
- `collections.namedtuple` - Uses empty `__slots__ = ()` to prevent new attributes
- `dataclasses` with `slots=True` - Modern way to create memory-efficient data classes
- `inspect.Signature` - Uses `__slots__ = ('_return_annotation', '_parameters')`

**Third-Party Libraries:**
- Many data science libraries (pandas, numpy) use `__slots__` for performance-critical classes
- ORM libraries often use `__slots__` for model classes
- Game engines and scientific computing libraries for memory efficiency

### Trade-offs of `__slots__`

**Advantages:**
- Reduced memory usage
- Faster attribute access
- Prevents accidental attribute creation

**Disadvantages:**
- Can't add new attributes dynamically
- More complex inheritance (can't easily mix with classes that don't use `__slots__`)
- No `__dict__` attribute (unless explicitly included in `__slots__`)

### When to Look for `__slots__`

Check for `__slots__` when:
- Working with data classes or immutable objects
- Performance is critical (many instances)
- You need to understand memory usage patterns
- Exploring libraries that handle large datasets

---

## üì¶ Package Metadata & Dependencies

Understanding a package's metadata helps you assess its maturity, compatibility, and ecosystem integration. This information is crucial for making architectural decisions and understanding potential conflicts.

```bash
# Shell commands
$ pip show requests              # Basic package info
$ pip show --verbose requests    # Detailed info including dependencies
```

Python's package metadata system provides programmatic access to the same information you get from `pip show`, but with much more detail and flexibility. The `pkg_resources` and `importlib.metadata` modules let you inspect package versions, dependencies, entry points, and other metadata that can reveal a library's architecture and integration points.

This metadata exploration is particularly valuable for understanding dependency chains, identifying potential version conflicts, and discovering hidden functionality like command-line tools or plugin systems that might not be obvious from the main API.

```python
# Python introspection
import pkg_resources
import importlib.metadata

# Version info
dist = pkg_resources.get_distribution("requests")
print(f"Version: {dist.version}")
print(f"Location: {dist.location}")

# Dependencies
try:
    metadata = importlib.metadata.metadata("requests")
    print("Dependencies:", metadata.get_all("Requires-Dist"))
except importlib.metadata.PackageNotFoundError:
    print("Package metadata not found")

# Find all entry points (CLI commands, plugins, etc.)
entry_points = pkg_resources.get_entry_map("requests")
print("Entry points:", entry_points)
```

Dependencies reveal the library's ecosystem and potential version conflicts. Entry points show you if the package provides command-line tools or plugin interfaces - valuable for understanding the full scope of functionality.

---

## üé® Pretty-Print & Visualization

Complex data structures and nested objects can be overwhelming when printed with default Python formatting. These techniques help you visualize library structures and data in human-readable formats, making patterns and relationships more obvious.

```python
from pprint import pprint
import json

# Create some example data to pretty-print
data = {
    'requests': {
        'classes': ['Session', 'Response', 'Request'],
        'functions': ['get', 'post', 'put', 'delete'],
        'config': {
            'timeout': 30,
            'verify': True,
            'allow_redirects': True
        }
    }
}

# Pretty print complex structures
pprint(data, width=80, depth=3)
```

The `json.dumps()` function converts Python objects to a JSON string representation, making complex data structures readable and portable. The `indent=2` parameter creates nicely formatted, human-readable output, while `default=str` handles non-serializable objects by converting them to strings. The `print_module_tree()` function creates a hierarchical tree view of a module's structure, showing classes, functions, and their methods in an organized format. This visualization helps you quickly understand the library's architecture, identify the main components, and spot patterns in naming conventions that reveal the library's design principles.

```python
# JSON formatting for serializable data
print(json.dumps(data, indent=2, default=str))

# Tree-like visualization of module structure
def print_module_tree(module, max_depth=2, current_depth=0):
    """Print a tree view of module structure"""
    if current_depth >= max_depth:
        return
    
    indent = "  " * current_depth
    for name, obj in inspect.getmembers(requests):
        if not name.startswith('_'):
            obj_type = type(obj).__name__
            print(f"{indent}‚îú‚îÄ‚îÄ {name} ({obj_type})")
            
            if inspect.isclass(obj) and current_depth < max_depth - 1:
                for method_name, method in inspect.getmembers(obj, inspect.ismethod):
                    if not method_name.startswith('_'):
                        print(f"{indent}‚îÇ   ‚îú‚îÄ‚îÄ {method_name}()")

print_module_tree(requests)
```

The tree visualization is particularly valuable for understanding hierarchical relationships and spotting naming patterns that reveal the library's organizational principles.

---

## üß† Deep Inspection

The `inspect` module is your Swiss Army knife for understanding Python objects at a deeper level. It reveals function signatures, source code, and type information that helps you write more robust code and understand performance implications.

```python
import inspect

# Get all members with their types
inspect.getmembers(requests)          # List all members
inspect.getsource(requests.get)       # Get source code
inspect.signature(requests.get)       # Show function signature

# Type checking for defensive programming. Like most/all *.isXYZ functions, these return a boolean
inspect.isfunction(obj)                   # Check if object is a function
inspect.isclass(obj)                      # Check if object is a class
inspect.ismodule(obj)                     # Check if object is a module
inspect.ismethod(obj)                     # Check if object is a method

# Parameter introspection for safe calling
sig = inspect.signature(requests.get)
for param_name, param in sig.parameters.items():
    print(f"{param_name}: {param.annotation}, default={param.default}")
```

Function signatures are particularly valuable because they show you not just what parameters are required, but also type hints and default values. This information is essential for writing defensive code that handles edge cases gracefully.

### Adding Deep Inspection To Your Own Code

To make your own functions work with `inspect.signature()` and other introspection tools, you need to understand how Python's signature system works. Here are the key approaches:

#### 1. Automatic Signature Detection (Most Common)

Python automatically detects signatures for most functions with type hints:

```python
import inspect

def my_library_function(name: str, age: int = 25, city: str = "NYC") -> str:
    """A function with type hints and defaults"""
    return f"{name} is {age} from {city}"

# inspect.signature() works automatically
sig = inspect.signature(my_library_function)
print(sig)  # (name: str, age: int = 25, city: str = "NYC") -> str

# Get parameter details
for param_name, param in sig.parameters.items():
    print(f"{param_name}: {param.annotation}, default={param.default}")
```

#### 2. Using `__annotations__` for Type Hints

```python
def my_dynamic_function(x, y, z):
    """Function without type hints in signature"""
    pass

# Add type hints after definition
my_dynamic_function.__annotations__ = {
    'x': int,
    'y': str, 
    'z': list,
    'return': bool
}

# Now inspect.signature() will show the types
sig = inspect.signature(my_dynamic_function)
print(sig)  # (x: int, y: str, z: list) -> bool
```

#### 3. Using `functools.wraps` for Decorators

```python
import functools
import inspect

def my_decorator(func):
    @functools.wraps(func)  # This preserves the original signature
    def wrapper(*args, **kwargs):
        print(f"Calling {func.__name__}")
        return func(*args, **kwargs)
    return wrapper

@my_decorator
def decorated_function(name: str, count: int = 1) -> str:
    return name * count

# inspect.signature() works correctly
sig = inspect.signature(decorated_function)
print(sig)  # (name: str, count: int = 1) -> str
```

#### 4. Custom Signature with `inspect.Signature`

For advanced cases, you can create custom signatures:

```python
import inspect
from inspect import Parameter

def my_dynamic_function(*args, **kwargs):
    """Function with dynamic behavior"""
    pass

# Create a custom signature
params = [
    Parameter('name', Parameter.POSITIONAL_OR_KEYWORD, annotation=str),
    Parameter('age', Parameter.POSITIONAL_OR_KEYWORD, default=25, annotation=int),
    Parameter('city', Parameter.KEYWORD_ONLY, default="NYC", annotation=str)
]

# Attach the signature
my_dynamic_function.__signature__ = inspect.Signature(
    parameters=params,
    return_annotation=str
)

# Now inspect.signature() works
sig = inspect.signature(my_dynamic_function)
print(sig)  # (name: str, age: int = 25, *, city: str = 'NYC') -> str
```

#### 5. Best Practices for Library Functions

```python
import inspect
from typing import Optional, List, Dict, Any

def my_library_api(
    endpoint: str,
    data: Optional[Dict[str, Any]] = None,
    headers: Optional[Dict[str, str]] = None,
    timeout: float = 30.0
) -> Dict[str, Any]:
    """
    A well-documented library function that works great with inspect.signature()
    
    Args:
        endpoint: The API endpoint to call
        data: Optional data to send
        headers: Optional headers to include
        timeout: Request timeout in seconds
    
    Returns:
        Response data as dictionary
    """
    # Implementation here
    pass

# This automatically works with inspect.signature()
sig = inspect.signature(my_library_api)
print(sig)
# (endpoint: str, data: Optional[Dict[str, Any]] = None, 
#  headers: Optional[Dict[str, str]] = None, timeout: float = 30.0) -> Dict[str, Any]
```

#### Key Points for Your Own Code:

1. **Type hints are automatically detected** by `inspect.signature()`
2. **Default values are preserved** in the signature
3. **Return type annotations** are included
4. **`functools.wraps`** preserves signatures in decorators
5. **Custom signatures** can be created for special cases

The most common and recommended approach is to use **type hints** in your function definitions - Python will automatically make them work with `inspect.signature()` and other introspection tools.

---

## üèóÔ∏è Domain Model Discovery

Understanding a library's domain model is crucial for writing code that aligns with the library's intended patterns. The domain model represents how the library thinks about the problem space - what are the core entities, what operations are available, and how do they relate to each other.

Domain-driven design principles apply to library exploration: you want to identify the ubiquitous language (naming conventions), bounded contexts (module boundaries), and core abstractions (main classes and interfaces). This understanding helps you write code that feels natural and maintainable.

```python
# Find the core abstractions and entities
classes = [name for name, obj in inspect.getmembers(requests, inspect.isclass)]
functions = [name for name, obj in inspect.getmembers(requests, inspect.isfunction)]
constants = [name for name in dir(requests) if name.isupper()]

print("Core Classes (Domain Entities):", classes)
print("Operations (Domain Services):", functions) 
print("Configuration/Constants:", constants)

# Explore class hierarchies to understand domain relationships
for cls_name in classes:
    cls = getattr(requests, cls_name)
    print(f"{cls_name}: {cls.__mro__}")  # Method Resolution Order shows inheritance
```

### Model Context Protocol Example:
```python
import mcp
# Discover the MCP domain abstractions
protocol_classes = [name for name, obj in inspect.getmembers(mcp, inspect.isclass) 
                    if any(term in name.lower() for term in ['server', 'client', 'transport', 'handler'])]
print("Protocol Domain Objects:", protocol_classes)

# Understand the communication patterns
transport_types = [name for name in dir(mcp) if 'transport' in name.lower()]
message_types = [name for name in dir(mcp) if 'message' in name.lower() or 'request' in name.lower()]
print("Transport mechanisms:", transport_types)
print("Message types:", message_types)
```

Class hierarchies (shown by `__mro__`) reveal inheritance relationships and help you understand which classes are specializations of others. Constants often represent configuration options or enumerated values that are important for proper usage.

---

## üõ°Ô∏è Defensive Exploration & Error Discovery

Defensive programming starts with understanding what can go wrong. By discovering a library's exception hierarchy and testing edge cases early, you can write more robust code and handle errors gracefully.

```python
# Discover what exceptions a library defines
exceptions = [name for name, obj in inspect.getmembers(requests, 
             lambda x: inspect.isclass(x) and issubclass(x, Exception))]
print("Custom Exceptions:", exceptions)

# Test parameter validation safely
def safe_explore_function(func, test_args=None):
    """Safely test a function with various inputs"""
    sig = inspect.signature(func)
    try:
        if not test_args:
            # Try calling with no args if no required params
            required_params = [p for p in sig.parameters.values() 
                             if p.default == inspect.Parameter.empty]
            if not required_params:
                result = func()
                print(f"SUCCESS {func.__name__}() -> {type(result)}")
        else:
            result = func(*test_args)
            print(f"SUCCESS {func.__name__}{test_args} -> {type(result)}")
    except Exception as e:
        print(f"ERROR {func.__name__}: {type(e).__name__}: {e}")

# Example usage
safe_explore_function(requests.get, ("https://httpbin.org/get",))
```

Custom exceptions tell you what the library considers to be error conditions and how it expects you to handle them. The safe exploration function helps you test library functions without crashing your exploration session.

### MCP Error Handling Example:
```python
import mcp
# Discover MCP-specific error handling patterns
mcp_exceptions = [name for name, obj in inspect.getmembers(mcp, 
                 lambda x: inspect.isclass(x) and issubclass(x, Exception))]
print("MCP Error types:", mcp_exceptions)

# Look for error handling patterns
error_handlers = [name for name in dir(mcp) if 'error' in name.lower() or 'exception' in name.lower()]
print("Error handling utilities:", error_handlers)
```

---

##  Usage Pattern Discovery

Libraries often follow common patterns that aren't immediately obvious from their API documentation. By analyzing naming conventions, method patterns, and docstring examples, you can discover the idiomatic ways to use the library.

```python
# syntax test: force reparse
# Find example usage in docstrings
import re
import inspect

def find_usage_examples(module):
    """Extract code examples from docstrings"""
    pass

# find_usage_examples(requests)
```

```python
# syntax test: force reparse
# Complete exploration template
import inspect
import importlib

def explore_package(package_name):
    """Complete package exploration workflow"""
    pass

# Usage examples
# explore_package('requests')
# explore_package('json')
# explore_package('webbrowser')
```

---

##  Performance Introspection

Understanding the performance characteristics of library functions helps you make informed decisions about when and how to use them. Simple profiling during exploration can reveal performance bottlenecks before they become problems in production.

```python
import timeit
import sys
import tracemalloc

# Quick performance check
def profile_function(func, *args, **kwargs):
    """Quick performance profiling of a function"""
    # Time it
    time_taken = timeit.timeit(lambda: func(*args, **kwargs), number=1000)
    print(f"TIMING {func.__name__}: {time_taken:.4f}s for 1000 calls")
    
    # Memory usage
    tracemalloc.start()
    result = func(*args, **kwargs)
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    print(f"MEMORY: {current / 1024:.2f} KB current, {peak / 1024:.2f} KB peak")
    
    return result

# Check object sizes
def object_size_analysis(obj):
    """Analyze memory footprint of objects"""
    print(f"Object size: {sys.getsizeof(obj)} bytes")
    if hasattr(obj, '__dict__'):
        print(f"Dict size: {sys.getsizeof(obj.__dict__)} bytes")
        print(f"Attributes: {len(obj.__dict__)}")

# Example usage
# profile_function(requests.get, "https://httpbin.org/get")
```

Memory profiling is particularly important for libraries that create large objects or process significant amounts of data. Object size analysis helps you understand the memory overhead of different approaches.

---

##  Interactive Exploration Workflow

Having a systematic workflow for exploration ensures you don't miss important aspects of a library and helps you build understanding progressively from high-level concepts to specific implementation details.

```python
# syntax test: force reparse
# Complete exploration template
import inspect
import importlib

def explore_package(package_name):
    """Complete package exploration workflow"""
    pass

# Usage examples
# explore_package('requests')
# explore_package('json')
# explore_package('webbrowser')
```

This workflow template provides a consistent starting point for any library exploration. It builds understanding progressively and gives you concrete next steps for deeper investigation.

---

##  Pro Tips for Productive Exploration

### Use IPython Magic
```bash
pip install ipython rich
ipython
```

IPython transforms your exploration experience with enhanced introspection capabilities, tab completion, and rich formatting. The `%timeit` magic command is particularly useful for quick performance testing, while `%pdb` drops you into the debugger when exceptions occur.

### Quick Testing Patterns
```python
# Create a sandbox namespace for safe experimentation
import types
sandbox = types.ModuleType('sandbox')

# Test configurations safely
test_configs = [
    {'timeout': 5},
    {'timeout': 10, 'verify': False},
    {}  # Empty config
]

for config in test_configs:
    try:
        session = requests.Session()
        # Test session configuration
        for key, value in config.items():
            setattr(session, key, value)
        print(f"SUCCESS Config {config} -> Session configured")
    except Exception as e:
        print(f"ERROR Config {config} -> {type(e).__name__}: {e}")
```

The sandbox pattern is particularly useful when you're not sure if library functions have side effects. Testing multiple configurations quickly reveals the flexibility and constraints of the library's interfaces.

### MCP Specific Exploration
```python
# Discover MCP server/client patterns
import mcp

# Find builder/factory patterns for MCP components
builders = [name for name in dir(mcp) if any(term in name.lower() 
           for term in ['builder', 'factory', 'create'])]
print("MCP builders:", builders)

# Explore server and client abstractions
server_types = [name for name in dir(mcp) if 'server' in name.lower()]
client_types = [name for name in dir(mcp) if 'client' in name.lower()]
print("Server types:", server_types)
print("Client types:", client_types)

# Understand transport and protocol layers
transport_classes = [name for name, obj in inspect.getmembers(mcp, inspect.isclass)
                    if 'transport' in name.lower()]
protocol_classes = [name for name, obj in inspect.getmembers(mcp, inspect.isclass)
                   if 'protocol' in name.lower()]
print("Transport layer:", transport_classes)
print("Protocol layer:", protocol_classes)
```

---

##  Conclusion

Effective library exploration is about building understanding systematically, from high-level domain concepts to specific implementation details. By following this guide, you've learned to:

**Understand the Structure**: Basic introspection and documentation exploration give you the lay of the land, while dunder attributes reveal the object's identity and contracts.

**Map the Domain**: Domain model discovery helps you understand how the library thinks about the problem space, enabling you to write code that aligns with the library's intended patterns.

**Explore Safely**: Defensive exploration techniques help you understand error conditions and edge cases without breaking your exploration session.

**Discover Patterns**: Usage pattern analysis reveals idiomatic approaches and common workflows that may not be obvious from documentation alone.

**Assess Performance**: Performance introspection helps you make informed decisions about when and how to use different library features.

**Work Systematically**: The interactive exploration workflow ensures comprehensive coverage and gives you a repeatable process for any new library.

The key insight is that library exploration is not just about learning the API - it's about understanding the library's mental model and design philosophy. When you understand how a library thinks about the problem domain, you can write code that feels natural, performs well, and handles edge cases gracefully.

Remember to start broad with the exploration workflow, then dive deep into the specific classes and functions that map to your domain needs. The REPL is your laboratory for understanding not just what the library can do, but how it expects to be used.

**Happy exploring!** üêç‚ú®

---

## üõ†Ô∏è Automation Tools

This project includes several automation tools to convert the README into different formats for enhanced learning and documentation.

### üìì Interactive Jupyter Notebook

The README can be automatically converted to an interactive Jupyter notebook where:
- **Python code blocks** become executable code cells
- **Shell/bash commands** become terminal cells using `%%bash`